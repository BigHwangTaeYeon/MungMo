[kafka 데이터베이스 동기화](#kafka)
[스케쥴러 vs 쿼츠](#scheduler-vs-quartz)

### kafka

Kafka 사용 이유
서비스 간의 통신
기존에는 RestApi 통신으로, 커플링되어 응답 서버에 문제가 있으면 요청한 서버에서 진행이 안되기에 디커플링이 필요하다.
(MSA 의 의존성 낮추는 장점이 무너짐)
I/O 데이터 구조 변경이 어려움
(연결되는 서비스가 모두 다 같이 변경 해야한다.)
부하분산 필요
메시지 큐(Rabbit MQ, Active MQ)
메모리에서 관리하고 반환하면 기존 데이터를 삭제한다.(여러 서비스에 전송 불가)
낮은 처리량(through put)
높은 레이턴시(latency)

그래서 이벤트 기반 MSA 아키텍쳐 Apache kafka 사용

REST API 호출이 적합한 경우:
실시간 데이터 접근이 필요할 때
동기적 처리와 강한 일관성이 필요할 때
이벤트 기반 통신이 적합한 경우:
비동기적 처리가 가능하고 일관성 요구가 낮을 때
서비스 간의 결합도를 낮추고 확장성을 높이고 싶을 때





    환경
kafka_2.13-3.7.0
connect : curl -O http://packages.confluent.io/archive/7.5/confluent-community-7.5.0.tar.gz
connector : curl -O https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-jdbc/versions/10.6.4/confluentinc-kafka-connect-jdbc-10.6.4.zip

    실행
$ bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
$ bin/kafka-server-start.sh -daemon config/server.properties
$ bin/connect-distributed -daemon ./etc/kafka/connect-distributed.properties

bin/connect-distributed ./etc/kafka/connect-distributed.properties

Zookeeper: 2181
Kafka Broker: 9092
Kafka Connect REST API: 8083




$ bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
$ bin/kafka-server-start.sh -daemon config/server.properties

Zookeeper 의 기본 포트는 2181이고 Kafka 의 기본 포트는 9092
$ lsof -i :9092     (pid 확인)
$ kill -9 1523

$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic HereTopicName --partitions 1
$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topicname --partitions 1 --replication-factor 1

$ bin/kafka-topics.sh --delete --topic HereTopicName --bootstrap-server localhost:9092

Producer 등록
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic HereTopicName
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicname

Consumer 등록
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic HereTopicName --from-beginning
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topicname --from-beginning




connect
$ bin/connect-distributed -daemon ./etc/kafka/connect-distributed.properties
$ lsof -i :8083

mariadb-java-client-2.7.2.jar 파일을 kafka connect → share/java/kafka로 복사함

connect-distributed.properties 파일은 Apache Kafka 의 Connect 분산 모드에서 사용되는 설정 파일입니다.
```properties
plugin.path=/usr/share/java
```
위 경로를 jdbc.jar 파일 경로로 변경해야한다.
```properties
plugin.path=/Users/userName/Documents/workspace/MungMo/confluentinc-kafka-connect-jdbc-10.6.4/lib/
```



1. 토픽 생성
   POST localhost:8083/connectors
   {
   "name" : "test-connect-mungmo-test001",
   "config" : {
   "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
   "connection.url":"jdbc:mysql://localhost:3306/mungMo",
   "connection.user":"mungmo",
   "connection.password":"1234",
   "mode": "incrementing",
   "incrementing.column.name" : "seq",
   "table.whitelist":"test001",
   "topic.prefix" : "my_topic_",
   "tasks.max" : "1"
   }
   }

2. sink connect 생성
   POST localhost:8083/connectors
   {
   "name":"my-sink-connect-member-test001",
   "config":{
   "connector.class":"io.confluent.connect.jdbc.JdbcSinkConnector",
   "connection.url":"jdbc:mysql://localhost:3306/mungmoMember",
   "connection.user":"mungmo",
   "connection.password":"1234",
   "auto.create":"true",
   "auto.evolve":"true",
   "delete.enabled":"false",
   "tasks.max":"1",
   "topics":"my_topic_test001"
   }
   }

3. table 또는 데이터 생성 수정 삭제
   mungMo 데이터베이스에서 dml 에 의해 mungmoMember 에 데이터 복제



topic 삭제를 하여면 먼저 생성한 connector 부터 삭제해야한다.

커넥터 삭제 : DELETE http://localhost:8083/connectors/my-sink-connect-member

토픽 삭제 : bin/kafka-topics.sh --delete --topic my_topic_test001 --bootstrap-server localhost:9092

확인 : bin/kafka-topics.sh --bootstrap-server localhost:9092 --list


==========================================================================================

### Scheduler vs Quartz

1. 동네인증
한달에 한번 재인증을 해야한다.
유저 데이터의 인증 여부, 인증일을 확인하여 30일 이후 동네인증 취소를 해야한다.

1분 단위 작업을 ShedLock 으로 중복 실행되지 않도록 구현하였다.

스케쥴, 쿼츠 고민은 작업이 너무 간단하여 스케쥴로 결정

2. 24시 동물병원


==========================================================================================

rest api vs kafka cdc
데이터 정합


































